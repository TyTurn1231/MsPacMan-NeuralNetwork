{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA_ef91YLBPD"
      },
      "source": [
        "#Ty Turner - Math 5366 - Data Science 2\n",
        "#Using a DQN to simulate an Agent playing Atari-PacMan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J84i2IAVKiRB",
        "outputId": "d49c08b7-0ca7-4ed2-ae1f-e7037b45f678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting gymnasium[accept-rom-license,atari]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale-py>=0.9 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, gymnasium, ale-py, autorom\n",
            "Successfully installed ale-py-0.10.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy torch torchvision opencv-python pillow\n",
        "!pip install --upgrade gymnasium[atari,accept-rom-license] autorom\n",
        "!pip install --upgrade ale-py\n",
        "!AutoROM --accept-license\n",
        "\n",
        "# Imports\n",
        "import gym\n",
        "import pygame\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import os\n",
        "import shutil\n",
        "import imageio\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eT09ucxVq9-",
        "outputId": "1cd72b75-0b28-4e61-e9b9-e045b75be089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaoDZGiKJ0iv"
      },
      "outputs": [],
      "source": [
        "# Deep Q-Network Class\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        num_input_channels = input_shape[0]  # stack_size × channels\n",
        "        self.conv1 = nn.Conv2d(num_input_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        return self.fc(x)\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Predict the best action for a given state.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():  # Disable gradient computation for inference\n",
        "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "            q_values = self(state)  # Get Q-values from the model\n",
        "            action = torch.argmax(q_values).item()  # Select the action with the highest Q-value\n",
        "        return action\n",
        "\n",
        "    def save_model(self, filepath, optimizer=None):\n",
        "        \"\"\"\n",
        "        Saves the model's state dict and optionally the optimizer's state dict.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None\n",
        "        }, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQXwTCXseqlV"
      },
      "outputs": [],
      "source": [
        "class CustomPacmanEnv(gym.Env):\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()  # Initialize the base Gym environment\n",
        "        self.render_mode = render_mode\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)\n",
        "        self.action_space = gym.spaces.Discrete(5)  # Example action space for Pacman, adjust as needed\n",
        "\n",
        "        # Load the base environment from the ALE/Pacman-v5\n",
        "        self.base_env = gym.make(\"MsPacman-v4\", render_mode=render_mode)\n",
        "        self.event = None  # Initialize the event attribute\n",
        "        self.previous_nearest_reward_distance = float('inf')  # Initialize previous distance as infinity\n",
        "        self.previous_pacman_position = None  # Initialize previous position as None\n",
        "        self.steps_since_last_event = 0  # Counter for steps since the last event\n",
        "        self.max_inactive_steps = 100  # Threshold for inactivity\n",
        "\n",
        "    def reset(self):\n",
        "        # Call the reset method of the base environment\n",
        "        state, info = self.base_env.reset()\n",
        "        self.event = None  # Reset the event attribute\n",
        "        self.previous_nearest_reward_distance = float('inf')  # Reset the previous distance\n",
        "        self.previous_pacman_position = self.get_pacman_position(state)  # Set the initial position\n",
        "        self.steps_since_last_event = 0  # Reset inactivity counter\n",
        "        return state, info\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, _, done, truncated, info = self.base_env.step(action)\n",
        "\n",
        "        # Detect events\n",
        "        self.event = self.detect_event(next_state, info)\n",
        "        current_nearest_reward_distance = self.calculate_nearest_reward_distance(next_state)\n",
        "\n",
        "        # Update inactivity counter\n",
        "        if self.event:  # Reset counter if an event occurs\n",
        "            self.steps_since_last_event = 0\n",
        "        else:  # Increment counter if no event\n",
        "            self.steps_since_last_event += 1\n",
        "\n",
        "        # Check for reward collection and update the state\n",
        "        pacman_position = self.get_pacman_position(next_state)\n",
        "        if pacman_position is not None:\n",
        "            # Check if Pacman is on a reward\n",
        "            reward_positions = self.get_reward_positions(next_state)\n",
        "            if any(np.array_equal(pacman_position, reward_pos) for reward_pos in reward_positions):\n",
        "                # Remove the reward from the state\n",
        "                next_state[pacman_position[0], pacman_position[1]] = 0\n",
        "\n",
        "        # Calculate the number of remaining rewards\n",
        "        remaining_rewards = len(self.get_reward_positions(next_state))\n",
        "        info[\"rewards_remaining\"] = remaining_rewards  # Add this to the info dictionary\n",
        "\n",
        "        # Calculate custom reward\n",
        "        current_nearest_reward_distance = self.calculate_nearest_reward_distance(next_state)\n",
        "        reward = self.custom_reward(current_nearest_reward_distance, next_state)\n",
        "\n",
        "\n",
        "        # Check for inactivity penalty\n",
        "        if self.steps_since_last_event > self.max_inactive_steps:\n",
        "            reward -= .1  # Apply penalty for inactivity\n",
        "\n",
        "        # Update previous distance and position\n",
        "        self.previous_nearest_reward_distance = current_nearest_reward_distance\n",
        "        self.previous_pacman_position = self.get_pacman_position(next_state)\n",
        "\n",
        "        return next_state, reward, done, truncated, info\n",
        "\n",
        "    def detect_event(self, state, info):\n",
        "        \"\"\"\n",
        "        Detects and returns the current event based on state and info.\n",
        "        \"\"\"\n",
        "        # Example pseudo-code for detecting events\n",
        "        if \"pellets_remaining\" in info and info[\"pellets_remaining\"] < self.previous_pellets:\n",
        "            return \"eat_pellet\"\n",
        "        elif \"power_pellet_eaten\" in info and info[\"power_pellet_eaten\"]:\n",
        "            return \"eat_power_pellet\"\n",
        "        elif \"ghost_eaten\" in info and info[\"ghost_eaten\"]:\n",
        "            return \"eat_ghost\"\n",
        "        elif \"caught_by_ghost\" in info and info[\"caught_by_ghost\"]:\n",
        "            return \"caught_by_ghost\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_ghost_positions(state):\n",
        "        \"\"\"\n",
        "        Extract positions of all ghosts from the state.\n",
        "        Assumes ghosts have a specific identifier in the state (e.g., 4).\n",
        "        \"\"\"\n",
        "        ghost_positions = np.argwhere(state == 4)  # Assuming ghosts are represented by the value 4\n",
        "        return ghost_positions\n",
        "\n",
        "    def calculate_nearest_reward_distance(self, state):\n",
        "        \"\"\"\n",
        "        Calculate the distance from Pacman to the nearest reward.\n",
        "        \"\"\"\n",
        "        # Extract Pacman position and reward positions from the state\n",
        "        pacman_position = self.get_pacman_position(state)\n",
        "        reward_positions = self.get_reward_positions(state)\n",
        "\n",
        "        if not reward_positions:  # If there are no rewards left, return a high value\n",
        "            return float('inf')\n",
        "\n",
        "        # Calculate distances to all rewards\n",
        "        distances = [\n",
        "            np.linalg.norm(np.array(pacman_position) - np.array(reward_position))\n",
        "            for reward_position in reward_positions\n",
        "        ]\n",
        "\n",
        "        # Return the distance to the nearest reward\n",
        "        return min(distances)\n",
        "\n",
        "    def get_pacman_position(self, state):\n",
        "        \"\"\"\n",
        "        Extract Pacman's position from the state.\n",
        "        Assumes the state is a grid or image where Pacman has a unique identifier.\n",
        "        \"\"\"\n",
        "        pacman_position = np.argwhere(state == 3)  # Example: Assume Pacman is represented by the value 3\n",
        "        return pacman_position[0] if len(pacman_position) > 0 else None\n",
        "\n",
        "    def get_reward_positions(self, state):\n",
        "        \"\"\"\n",
        "        Extract positions of all rewards (e.g., pellets, power pellets) from the state.\n",
        "        Assumes rewards have specific identifiers in the state.\n",
        "        \"\"\"\n",
        "        reward_positions = np.argwhere((state == 1) | (state == 2))  # Example: Pellets = 1, Power Pellets = 2\n",
        "        return reward_positions\n",
        "\n",
        "    def custom_reward(self, current_distance, state):\n",
        "        \"\"\"\n",
        "        Assign rewards based on the current event, distance to nearest reward, and movement.\n",
        "        \"\"\"\n",
        "        reward = 0\n",
        "\n",
        "        # Event-based rewards\n",
        "        if self.event == \"eat_pellet\":\n",
        "            reward += 75\n",
        "        elif self.event == \"eat_power_pellet\":\n",
        "            reward += 200\n",
        "        elif self.event == \"eat_ghost\":\n",
        "            reward += 200\n",
        "        elif self.event == \"collect_fruit\":\n",
        "            reward += 500\n",
        "        elif self.event == \"clear_maze\":\n",
        "            reward += 10000\n",
        "        elif self.event == \"caught_by_ghost\":\n",
        "            reward -= 500\n",
        "\n",
        "        # Closeness-based reward\n",
        "        if current_distance != float('inf'):\n",
        "            distance_change = self.previous_nearest_reward_distance - current_distance\n",
        "            if distance_change > 0:  # Pacman moved closer to the reward\n",
        "                reward += 8.0\n",
        "            elif distance_change < 0:  # Pacman moved further from the reward\n",
        "                reward -= 5.0  # Penalize slightly for moving away\n",
        "\n",
        "        # Penalize lack of movement\n",
        "        if self.previous_pacman_position is not None:\n",
        "            if np.array_equal(self.previous_pacman_position, self.get_pacman_position(state)):\n",
        "                reward -= 2\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        # Delegate rendering to the base environment\n",
        "        return self.base_env.render(mode)\n",
        "\n",
        "    def close(self):\n",
        "        # Close the base environment\n",
        "        self.base_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v6fzP_hPDyt"
      },
      "outputs": [],
      "source": [
        "# Preprocess frame using PyTorch transforms (as you described)\n",
        "def preprocess_frame(frame):\n",
        "    transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Grayscale(num_output_channels=1),  # Ensure single channel\n",
        "        T.Resize((84, 84)),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "    return transform(frame)  # Shape: [1, 84, 84]\n",
        "\n",
        "# Stack multiple frames\n",
        "def stack_frames(frames, new_frame, stack_size=4):\n",
        "    if frames is None:\n",
        "        frames = []  # Initialize if None\n",
        "    frames.append(new_frame)\n",
        "    if len(frames) > stack_size:\n",
        "        frames = frames[-stack_size:]  # Keep the most recent `stack_size` frames\n",
        "    elif len(frames) < stack_size:\n",
        "        while len(frames) < stack_size:\n",
        "            frames.append(new_frame)  # Pad with the current frame\n",
        "    stacked_frames = torch.cat(frames, dim=0)  # Concatenate along the channel dim\n",
        "    return stacked_frames, frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTu-qy0-TBLD"
      },
      "outputs": [],
      "source": [
        "def adjust_model_to_action_space(model, n_actions):\n",
        "    \"\"\"\n",
        "    Adjusts the output layer of the model to match the number of actions.\n",
        "    \"\"\"\n",
        "    old_fc = model.fc[-1]  # Get the current output layer\n",
        "    if old_fc.out_features != n_actions:\n",
        "        print(f\"Adjusting model output layer from {old_fc.out_features} to {n_actions} actions.\")\n",
        "        model.fc[-1] = nn.Linear(old_fc.in_features, n_actions)\n",
        "    return model\n",
        "\n",
        "def safe_load_model(model, checkpoint_path, n_actions):\n",
        "    \"\"\"\n",
        "    Loads a model from the checkpoint, adjusting for changes in action space.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # Filter out mismatched keys (e.g., output layers with different shapes)\n",
        "    filtered_state_dict = {k: v for k, v in model_state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "\n",
        "    # Load the filtered state dictionary\n",
        "    model_dict.update(filtered_state_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Adjust the output layer if necessary\n",
        "    model = adjust_model_to_action_space(model, n_actions)\n",
        "\n",
        "    print(f\"Loaded model with adjustments for {n_actions} actions.\")\n",
        "    return model\n",
        "\n",
        "# Function to load or create a new model\n",
        "def initialize_model(input_shape, n_actions, optimizer=None, is_training=True):\n",
        "    model = DQN(input_shape, n_actions)\n",
        "    target_model = DQN(input_shape, n_actions)\n",
        "\n",
        "    # Default save directory if no model is loaded\n",
        "    save_dir = None\n",
        "\n",
        "    choice = input(\"Would you like to load a saved model? (yes/no): \").strip().lower()\n",
        "    task_count = int(input(\"Enter the number of episodes to train the model: \"))\n",
        "\n",
        "    if choice == \"yes\":\n",
        "        model_path = input(\"Enter the path to the saved model file: \").strip()\n",
        "        if os.path.isfile(model_path):\n",
        "            checkpoint = torch.load(model_path)  # Load the checkpoint here\n",
        "\n",
        "            model = safe_load_model(model, model_path, n_actions)\n",
        "\n",
        "            # Create a unique save directory for this session\n",
        "            if is_training:\n",
        "              base_name = os.path.splitext(os.path.basename(model_path))[0]\n",
        "              timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # Add timestamp\n",
        "              save_dir = os.path.join(os.path.dirname(model_path), f\"{base_name}_{timestamp}\")  # Create folder with timestamp\n",
        "\n",
        "              if not os.path.exists(save_dir):\n",
        "                  os.makedirs(save_dir)\n",
        "                  print(f\"Directory created at {save_dir}\")\n",
        "\n",
        "            if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                print(\"Model loaded successfully.\")\n",
        "            else:\n",
        "              print(\"Model failed.\")\n",
        "              sys.exit(1)\n",
        "            print(f\"Loaded model from {model_path}.\")\n",
        "        else:\n",
        "            print(f\"File not found at {model_path}. Starting with a new model.\")\n",
        "    else:\n",
        "        print(\"Starting with a new model.\")\n",
        "\n",
        "    # Synchronize the target model with the main model\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    return model, target_model, task_count, save_dir\n",
        "\n",
        "def load_model(filepath, model, optimizer=None):\n",
        "    \"\"\"\n",
        "    Loads the model's state dict and optionally the optimizer's state dict.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath, weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(\"Model loaded successfully.\")\n",
        "    else:\n",
        "      print(\"Model failed.\")\n",
        "      sys.exit(1)\n",
        "    print(f\"Model loaded from {filepath}\")\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0fgiS-iK9c2"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_dqn(dqn_model, target_model, memory, optimizer, batch_size, gamma, loss_fn):\n",
        "    if len(memory) < batch_size:\n",
        "        return  # Skip if there aren't enough samples\n",
        "\n",
        "    # Use the `sample` method of ReplayMemory\n",
        "    minibatch = memory.sample(batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "    # Convert to tensors\n",
        "    states = torch.cat([s.unsqueeze(0) for s in states])  # [batch_size, 4, 84, 84]\n",
        "    next_states = torch.cat([ns.unsqueeze(0) for ns in next_states])  # [batch_size, 4, 84, 84]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # [batch_size]\n",
        "    dones = torch.tensor(dones, dtype=torch.bool)  # [batch_size]\n",
        "    actions = torch.tensor(actions).view(-1, 1)  # [batch_size, 1]\n",
        "\n",
        "    # Calculate target Q-values\n",
        "    with torch.no_grad():\n",
        "        max_next_q_values = target_model(next_states).max(1)[0]  # Shape: [batch_size]\n",
        "        targets = rewards + (1 - dones.float()) * gamma * max_next_q_values\n",
        "\n",
        "    # Predicted Q-values for the actions taken\n",
        "    predicted_q_values = dqn_model(states)  # Shape: [batch_size, n_actions]\n",
        "    selected_q_values = predicted_q_values.gather(1, actions).squeeze(1)  # Shape: [batch_size]\n",
        "    # Compute loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(selected_q_values, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Function to trigger video recording based on the highest reward\n",
        "def best_episode_trigger(episode_id, reward, highest_reward):\n",
        "    if reward > highest_reward:\n",
        "        highest_reward = reward\n",
        "        print(f\"New highest reward: {reward}, recording episode {episode_id}\")\n",
        "        return True  # Record this episode\n",
        "    return False  # Skip recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iLu6xW9CUq7"
      },
      "outputs": [],
      "source": [
        "# Function to clear the contents of a folder\n",
        "def clear_folder(folder_path):\n",
        "    # If the folder exists, delete all files inside it\n",
        "    if os.path.exists(folder_path):\n",
        "        shutil.rmtree(folder_path)\n",
        "    # Recreate the folder after clearing\n",
        "    os.makedirs(folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZrRXz9OK40f"
      },
      "outputs": [],
      "source": [
        "# Main Training Loop\n",
        "def train_pacman():\n",
        "    env = CustomPacmanEnv(render_mode=\"rgb_array\")  # Use the custom environment\n",
        "    n_actions = env.action_space.n\n",
        "    input_shape = (4, 84, 84)  # Stack size × resized frame dimensions\n",
        "    memory_capacity = 20000\n",
        "    memory = ReplayMemory(memory_capacity)\n",
        "\n",
        "    # Hyperparameters\n",
        "    gamma = 0.99\n",
        "    learning_rate = 0.00025\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.01\n",
        "    epsilon_decay = 0.995\n",
        "    batch_size = 32\n",
        "    n_episodes = 10\n",
        "\n",
        "    model = DQN(input_shape, n_actions)\n",
        "    target_model = DQN(input_shape, n_actions)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "    target_model.eval()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Load or create the model\n",
        "    is_training = True\n",
        "    model, target_model, n_episodes, file_loc = initialize_model(input_shape, n_actions, optimizer, is_training)\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = preprocess_frame(state)  # Preprocess the initial state\n",
        "        stacked_state, frame_stack = stack_frames(None, state, stack_size=4)  # Stack the frames\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Select an action\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(model(stacked_state.unsqueeze(0))).item()\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            done = done or truncated\n",
        "\n",
        "            # Preprocess and stack the next state\n",
        "            next_state = preprocess_frame(next_state)\n",
        "            stacked_next_state, frame_stack = stack_frames(frame_stack, next_state, stack_size=4)\n",
        "\n",
        "            # Store the transition in replay memory\n",
        "            memory.remember(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "            # Train the model using replay memory\n",
        "            train_dqn(model, target_model, memory, optimizer, batch_size, gamma, loss_fn)\n",
        "\n",
        "            # Update the current state\n",
        "            stacked_state = stacked_next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Update epsilon\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        # Update target model every 5 episodes\n",
        "        if (episode + 1) % 5 == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Save with timestamp every 25 episodes\n",
        "        if (episode + 1) & 25 == 0:\n",
        "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "            filename = file_loc + f'/PACMAN_dqn_model_{timestamp}.pth'\n",
        "            model.save_model(filename, optimizer)\n",
        "            print(f\"Model with timestamp saved after episode {episode+1}: {filename}\")\n",
        "\n",
        "        print(f\"Episode {episode+1}/{n_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_model(f'/content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth', optimizer)\n",
        "    env.close()\n",
        "\n",
        "def train_pacman_with_video():\n",
        "    env = CustomPacmanEnv(render_mode=\"rgb_array\")  # Use the custom environment\n",
        "    n_actions = env.action_space.n\n",
        "    input_shape = (4, 84, 84)  # Stack size × resized frame dimensions\n",
        "    memory_capacity = 20000\n",
        "    memory = ReplayMemory(memory_capacity)\n",
        "\n",
        "    # Hyperparameters\n",
        "    gamma = 0.99\n",
        "    learning_rate = 0.00025\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.01\n",
        "    epsilon_decay = 0.995\n",
        "    batch_size = 32\n",
        "    n_episodes = 10\n",
        "\n",
        "    model = DQN(input_shape, n_actions)\n",
        "    target_model = DQN(input_shape, n_actions)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "    target_model.eval()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Wrap the environment to record the video\n",
        "    env = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
        "\n",
        "    # Load or create the model\n",
        "    is_training = True\n",
        "    model, target_model, n_episodes, file_loc = initialize_model(input_shape, n_actions, optimizer, is_training)\n",
        "\n",
        "\n",
        "    video_folder = '/content/pacman_training_videos'  # Local folder to save all videos\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "    clear_folder(video_folder)\n",
        "\n",
        "    # Track the highest reward and corresponding episode\n",
        "    highest_reward = [float('-inf')]  # Use a list to store the mutable highest reward\n",
        "    best_episode_video_folder = \"/content/best_pacman_training_video\"\n",
        "    os.makedirs(best_episode_video_folder, exist_ok=True)\n",
        "    clear_folder(best_episode_video_folder)\n",
        "\n",
        "    best_episode_video_wrapper = None  # Initialize the video wrapper\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = preprocess_frame(state)  # Preprocess the initial state\n",
        "        stacked_state, frame_stack = stack_frames(None, state, stack_size=4)  # Stack the frames\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        # Initialize the video wrapper for the current episode\n",
        "        current_episode_video_wrapper = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda eid: eid == episode)\n",
        "        env = current_episode_video_wrapper  # Use the wrapper for recording the video\n",
        "\n",
        "        while not done:\n",
        "            # Select an action\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(model(stacked_state.unsqueeze(0))).item()\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            done = done or truncated\n",
        "\n",
        "            # Preprocess and stack the next state\n",
        "            next_state = preprocess_frame(next_state)\n",
        "            stacked_next_state, frame_stack = stack_frames(frame_stack, next_state, stack_size=4)\n",
        "\n",
        "            # Store the transition in replay memory\n",
        "            memory.remember(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "            # Train the model using replay memory\n",
        "            train_dqn(model, target_model, memory, optimizer, batch_size, gamma, loss_fn)\n",
        "\n",
        "            # Update the current state\n",
        "            stacked_state = stacked_next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Update epsilon\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        # Update target model every 5 episodes\n",
        "        if (episode + 1) % 5 == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Save with timestamp every 25 episodes\n",
        "        if (episode + 1) & 25 == 0:\n",
        "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "            filename = file_loc + f'/PACMAN_dqn_model_{timestamp}.pth'\n",
        "            model.save_model(filename, optimizer)\n",
        "            print(f\"Model with timestamp saved after episode {episode+1}: {filename}\")\n",
        "\n",
        "        # Update the video recording wrapper when a new highest reward is found\n",
        "        if total_reward > highest_reward[0]:\n",
        "            highest_reward[0] = total_reward  # Update the highest reward\n",
        "            print(f\"New highest reward: {total_reward}, recording episode {episode + 1}\")\n",
        "\n",
        "            if best_episode_video_wrapper:\n",
        "                best_episode_video_wrapper.close()  # Close the previous video recording wrapper\n",
        "\n",
        "            # Dynamically wrap the environment for the best episode only\n",
        "            best_episode_video_wrapper = gym.wrappers.RecordVideo(env, video_folder=best_episode_video_folder, episode_trigger=lambda eid: eid == episode)\n",
        "            env = best_episode_video_wrapper  # Use the updated wrapper for this episode\n",
        "\n",
        "        print(f\"Episode {episode+1}/{n_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_model(f'/content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth', optimizer)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp1MaI3fK6a0",
        "outputId": "9ee4ff3e-22ea-4a0e-9152-2a81965e83c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like to load a saved model? (yes/no): yes\n",
            "Enter the number of episodes to train the model: 200\n",
            "Enter the path to the saved model file: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-6b5b1b8d1d5a>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)  # Load the checkpoint here\n",
            "<ipython-input-37-6b5b1b8d1d5a>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n",
            "<ipython-input-35-0420cb395f80>:98: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if not reward_positions:  # If there are no rewards left, return a high value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model with adjustments for 5 actions.\n",
            "Directory created at /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01\n",
            "Model loaded successfully.\n",
            "Loaded model from /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth.\n",
            "Episode 1/200, Total Reward: -2855.7999999999706, Epsilon: 0.995\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-21-43.pth\n",
            "Model with timestamp saved after episode 2: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-21-43.pth\n",
            "Episode 2/200, Total Reward: -2417.1999999999784, Epsilon: 0.990025\n",
            "Episode 3/200, Total Reward: -2284.599999999981, Epsilon: 0.985074875\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-23-21.pth\n",
            "Model with timestamp saved after episode 4: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-23-21.pth\n",
            "Episode 4/200, Total Reward: -2361.0999999999794, Epsilon: 0.9801495006250001\n",
            "Episode 5/200, Total Reward: -2978.1999999999684, Epsilon: 0.9752487531218751\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-25-12.pth\n",
            "Model with timestamp saved after episode 6: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-25-12.pth\n",
            "Episode 6/200, Total Reward: -2570.1999999999757, Epsilon: 0.9703725093562657\n",
            "Episode 7/200, Total Reward: -3865.5999999999526, Epsilon: 0.9655206468094844\n",
            "Episode 8/200, Total Reward: -2345.7999999999797, Epsilon: 0.960693043575437\n",
            "Episode 9/200, Total Reward: -2616.099999999975, Epsilon: 0.9558895783575597\n",
            "Episode 10/200, Total Reward: -3309.6999999999625, Epsilon: 0.9511101304657719\n",
            "Episode 11/200, Total Reward: -2508.999999999977, Epsilon: 0.946354579813443\n",
            "Episode 12/200, Total Reward: -2804.7999999999715, Epsilon: 0.9416228069143757\n",
            "Episode 13/200, Total Reward: -2320.29999999998, Epsilon: 0.9369146928798039\n",
            "Episode 14/200, Total Reward: -3100.5999999999663, Epsilon: 0.9322301194154049\n",
            "Episode 15/200, Total Reward: -2605.899999999975, Epsilon: 0.9275689688183278\n",
            "Episode 16/200, Total Reward: -3003.699999999968, Epsilon: 0.9229311239742362\n",
            "Episode 17/200, Total Reward: -2447.799999999978, Epsilon: 0.918316468354365\n",
            "Episode 18/200, Total Reward: -1871.499999999988, Epsilon: 0.9137248860125932\n",
            "Episode 19/200, Total Reward: -2055.099999999985, Epsilon: 0.9091562615825302\n",
            "Episode 20/200, Total Reward: -3294.399999999963, Epsilon: 0.9046104802746175\n",
            "Episode 21/200, Total Reward: -2656.899999999974, Epsilon: 0.9000874278732445\n",
            "Episode 22/200, Total Reward: -1988.7999999999859, Epsilon: 0.8955869907338783\n",
            "Episode 23/200, Total Reward: -4120.59999999995, Epsilon: 0.8911090557802088\n",
            "Episode 24/200, Total Reward: -4630.599999999987, Epsilon: 0.8866535105013078\n",
            "Episode 25/200, Total Reward: -2524.2999999999765, Epsilon: 0.8822202429488013\n",
            "Episode 26/200, Total Reward: -2978.1999999999684, Epsilon: 0.8778091417340573\n",
            "Episode 27/200, Total Reward: -3248.4999999999636, Epsilon: 0.8734200960253871\n",
            "Episode 28/200, Total Reward: -2514.0999999999767, Epsilon: 0.8690529955452602\n",
            "Episode 29/200, Total Reward: -2090.7999999999843, Epsilon: 0.8647077305675338\n",
            "Episode 30/200, Total Reward: -1616.4999999999925, Epsilon: 0.8603841919146962\n",
            "Episode 31/200, Total Reward: -3396.399999999961, Epsilon: 0.8560822709551227\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-51-20.pth\n",
            "Model with timestamp saved after episode 32: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-51-20.pth\n",
            "Episode 32/200, Total Reward: -3100.5999999999663, Epsilon: 0.851801859600347\n",
            "Episode 33/200, Total Reward: -4610.199999999985, Epsilon: 0.8475428503023453\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-54-24.pth\n",
            "Model with timestamp saved after episode 34: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-54-24.pth\n",
            "Episode 34/200, Total Reward: -3845.199999999953, Epsilon: 0.8433051360508336\n",
            "Episode 35/200, Total Reward: -2151.999999999983, Epsilon: 0.8390886103705794\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-56-12.pth\n",
            "Model with timestamp saved after episode 36: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-56-12.pth\n",
            "Episode 36/200, Total Reward: -2233.5999999999817, Epsilon: 0.8348931673187264\n",
            "Episode 37/200, Total Reward: -4166.499999999954, Epsilon: 0.8307187014821328\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-58-46.pth\n",
            "Model with timestamp saved after episode 38: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_19-58-46.pth\n",
            "Episode 38/200, Total Reward: -2825.199999999971, Epsilon: 0.8265651079747222\n",
            "Episode 39/200, Total Reward: -3651.3999999999564, Epsilon: 0.8224322824348486\n",
            "Episode 40/200, Total Reward: -3003.699999999968, Epsilon: 0.8183201210226743\n",
            "Episode 41/200, Total Reward: -4635.699999999987, Epsilon: 0.8142285204175609\n",
            "Episode 42/200, Total Reward: -4477.599999999976, Epsilon: 0.810157377815473\n",
            "Episode 43/200, Total Reward: -3646.2999999999565, Epsilon: 0.8061065909263957\n",
            "Episode 44/200, Total Reward: -2809.8999999999714, Epsilon: 0.8020760579717637\n",
            "Episode 45/200, Total Reward: -3146.4999999999654, Epsilon: 0.798065677681905\n",
            "Episode 46/200, Total Reward: -2141.7999999999834, Epsilon: 0.7940753492934954\n",
            "Episode 47/200, Total Reward: -2656.899999999974, Epsilon: 0.7901049725470279\n",
            "Episode 48/200, Total Reward: -2514.0999999999767, Epsilon: 0.7861544476842928\n",
            "Episode 49/200, Total Reward: -3345.399999999962, Epsilon: 0.7822236754458713\n",
            "Episode 50/200, Total Reward: -3768.6999999999543, Epsilon: 0.778312557068642\n",
            "Episode 51/200, Total Reward: -2197.8999999999824, Epsilon: 0.7744209942832988\n",
            "Episode 52/200, Total Reward: -2519.1999999999766, Epsilon: 0.7705488893118823\n",
            "Episode 53/200, Total Reward: -2136.6999999999834, Epsilon: 0.7666961448653229\n",
            "Episode 54/200, Total Reward: -4258.29999999996, Epsilon: 0.7628626641409962\n",
            "Episode 55/200, Total Reward: -3936.9999999999513, Epsilon: 0.7590483508202912\n",
            "Episode 56/200, Total Reward: -3676.899999999956, Epsilon: 0.7552531090661897\n",
            "Episode 57/200, Total Reward: -4482.699999999976, Epsilon: 0.7514768435208588\n",
            "Episode 58/200, Total Reward: -3233.199999999964, Epsilon: 0.7477194593032545\n",
            "Episode 59/200, Total Reward: -2310.0999999999804, Epsilon: 0.7439808620067382\n",
            "Episode 60/200, Total Reward: -1478.799999999995, Epsilon: 0.7402609576967045\n",
            "Episode 61/200, Total Reward: -4125.699999999951, Epsilon: 0.736559652908221\n",
            "Episode 62/200, Total Reward: -3391.299999999961, Epsilon: 0.7328768546436799\n",
            "Episode 63/200, Total Reward: -2820.0999999999713, Epsilon: 0.7292124703704616\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-30-45.pth\n",
            "Model with timestamp saved after episode 64: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-30-45.pth\n",
            "Episode 64/200, Total Reward: -3406.599999999961, Epsilon: 0.7255664080186093\n",
            "Episode 65/200, Total Reward: -4069.599999999949, Epsilon: 0.7219385759785162\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-34-00.pth\n",
            "Model with timestamp saved after episode 66: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-34-00.pth\n",
            "Episode 66/200, Total Reward: -4712.1999999999925, Epsilon: 0.7183288830986236\n",
            "Episode 67/200, Total Reward: -4013.49999999995, Epsilon: 0.7147372386831305\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-37-09.pth\n",
            "Model with timestamp saved after episode 68: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-37-09.pth\n",
            "Episode 68/200, Total Reward: -4620.399999999986, Epsilon: 0.7111635524897149\n",
            "Episode 69/200, Total Reward: -3304.5999999999626, Epsilon: 0.7076077347272662\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-39-53.pth\n",
            "Model with timestamp saved after episode 70: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_20-39-53.pth\n",
            "Episode 70/200, Total Reward: -4125.699999999951, Epsilon: 0.7040696960536299\n",
            "Episode 71/200, Total Reward: -4972.300000000011, Epsilon: 0.7005493475733617\n",
            "Episode 72/200, Total Reward: -3090.3999999999664, Epsilon: 0.697046600835495\n",
            "Episode 73/200, Total Reward: -3115.899999999966, Epsilon: 0.6935613678313175\n",
            "Episode 74/200, Total Reward: -3345.399999999962, Epsilon: 0.6900935609921609\n",
            "Episode 75/200, Total Reward: -3248.4999999999636, Epsilon: 0.6866430931872001\n",
            "Episode 76/200, Total Reward: -4615.299999999986, Epsilon: 0.6832098777212641\n",
            "Episode 77/200, Total Reward: -3523.8999999999587, Epsilon: 0.6797938283326578\n",
            "Episode 78/200, Total Reward: -3906.399999999952, Epsilon: 0.6763948591909945\n",
            "Episode 79/200, Total Reward: -5161.000000000025, Epsilon: 0.6730128848950395\n",
            "Episode 80/200, Total Reward: -4421.499999999972, Epsilon: 0.6696478204705644\n",
            "Episode 81/200, Total Reward: -3151.5999999999653, Epsilon: 0.6662995813682115\n",
            "Episode 82/200, Total Reward: -4013.49999999995, Epsilon: 0.6629680834613705\n",
            "Episode 83/200, Total Reward: -2901.69999999997, Epsilon: 0.6596532430440636\n",
            "Episode 84/200, Total Reward: -4120.59999999995, Epsilon: 0.6563549768288433\n",
            "Episode 85/200, Total Reward: -2044.8999999999849, Epsilon: 0.653073201944699\n",
            "Episode 86/200, Total Reward: -3987.9999999999504, Epsilon: 0.6498078359349755\n",
            "Episode 87/200, Total Reward: -3314.7999999999624, Epsilon: 0.6465587967553006\n",
            "Episode 88/200, Total Reward: -4768.2999999999965, Epsilon: 0.6433260027715241\n",
            "Episode 89/200, Total Reward: -3131.1999999999657, Epsilon: 0.6401093727576664\n",
            "Episode 90/200, Total Reward: -1861.2999999999881, Epsilon: 0.6369088258938781\n",
            "Episode 91/200, Total Reward: -3921.6999999999516, Epsilon: 0.6337242817644086\n",
            "Episode 92/200, Total Reward: -2412.0999999999785, Epsilon: 0.6305556603555866\n",
            "Episode 93/200, Total Reward: -3753.3999999999546, Epsilon: 0.6274028820538087\n",
            "Episode 94/200, Total Reward: -5120.200000000022, Epsilon: 0.6242658676435396\n",
            "Episode 95/200, Total Reward: -3381.0999999999613, Epsilon: 0.6211445383053219\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-15-14.pth\n",
            "Model with timestamp saved after episode 96: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-15-14.pth\n",
            "Episode 96/200, Total Reward: -3559.599999999958, Epsilon: 0.6180388156137953\n",
            "Episode 97/200, Total Reward: -3238.299999999964, Epsilon: 0.6149486215357263\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-17-14.pth\n",
            "Model with timestamp saved after episode 98: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-17-14.pth\n",
            "Episode 98/200, Total Reward: -1947.9999999999866, Epsilon: 0.6118738784280476\n",
            "Episode 99/200, Total Reward: -3911.499999999952, Epsilon: 0.6088145090359074\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-19-56.pth\n",
            "Model with timestamp saved after episode 100: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-19-56.pth\n",
            "Episode 100/200, Total Reward: -3345.399999999962, Epsilon: 0.6057704364907278\n",
            "Episode 101/200, Total Reward: -5982.100000000083, Epsilon: 0.6027415843082742\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-23-50.pth\n",
            "Model with timestamp saved after episode 102: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_21-23-50.pth\n",
            "Episode 102/200, Total Reward: -4962.10000000001, Epsilon: 0.5997278763867329\n",
            "Episode 103/200, Total Reward: -4457.199999999974, Epsilon: 0.5967292370047992\n",
            "Episode 104/200, Total Reward: -4951.90000000001, Epsilon: 0.5937455908197752\n",
            "Episode 105/200, Total Reward: -3748.2999999999547, Epsilon: 0.5907768628656763\n",
            "Episode 106/200, Total Reward: -3630.999999999957, Epsilon: 0.5878229785513479\n",
            "Episode 107/200, Total Reward: -4809.099999999999, Epsilon: 0.5848838636585911\n",
            "Episode 108/200, Total Reward: -3462.69999999996, Epsilon: 0.5819594443402982\n",
            "Episode 109/200, Total Reward: -3549.3999999999583, Epsilon: 0.5790496471185967\n",
            "Episode 110/200, Total Reward: -2182.5999999999826, Epsilon: 0.5761543988830038\n",
            "Episode 111/200, Total Reward: -1738.8999999999903, Epsilon: 0.5732736268885887\n",
            "Episode 112/200, Total Reward: -3243.3999999999637, Epsilon: 0.5704072587541458\n",
            "Episode 113/200, Total Reward: -3421.8999999999605, Epsilon: 0.567555222460375\n",
            "Episode 114/200, Total Reward: -4135.899999999951, Epsilon: 0.5647174463480732\n",
            "Episode 115/200, Total Reward: -3432.0999999999603, Epsilon: 0.5618938591163328\n",
            "Episode 116/200, Total Reward: -3595.2999999999574, Epsilon: 0.5590843898207511\n",
            "Episode 117/200, Total Reward: -4120.59999999995, Epsilon: 0.5562889678716474\n",
            "Episode 118/200, Total Reward: -4538.79999999998, Epsilon: 0.5535075230322891\n",
            "Episode 119/200, Total Reward: -3676.899999999956, Epsilon: 0.5507399854171277\n",
            "Episode 120/200, Total Reward: -3411.6999999999607, Epsilon: 0.547986285490042\n",
            "Episode 121/200, Total Reward: -4299.099999999963, Epsilon: 0.5452463540625918\n",
            "Episode 122/200, Total Reward: -3799.299999999954, Epsilon: 0.5425201222922789\n",
            "Episode 123/200, Total Reward: -4645.899999999988, Epsilon: 0.5398075216808175\n",
            "Episode 124/200, Total Reward: -4707.099999999992, Epsilon: 0.5371084840724134\n",
            "Episode 125/200, Total Reward: -3845.199999999953, Epsilon: 0.5344229416520513\n",
            "Episode 126/200, Total Reward: -4028.7999999999497, Epsilon: 0.531750826943791\n",
            "Episode 127/200, Total Reward: -4533.69999999998, Epsilon: 0.5290920728090721\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-00-51.pth\n",
            "Model with timestamp saved after episode 128: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-00-51.pth\n",
            "Episode 128/200, Total Reward: -4258.29999999996, Epsilon: 0.5264466124450268\n",
            "Episode 129/200, Total Reward: -2763.9999999999723, Epsilon: 0.5238143793828016\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-03-16.pth\n",
            "Model with timestamp saved after episode 130: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-03-16.pth\n",
            "Episode 130/200, Total Reward: -3335.199999999962, Epsilon: 0.5211953074858876\n",
            "Episode 131/200, Total Reward: -2707.8999999999733, Epsilon: 0.5185893309484582\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-05-42.pth\n",
            "Model with timestamp saved after episode 132: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-05-42.pth\n",
            "Episode 132/200, Total Reward: -3738.099999999955, Epsilon: 0.5159963842937159\n",
            "Episode 133/200, Total Reward: -3885.9999999999523, Epsilon: 0.5134164023722473\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-08-13.pth\n",
            "Model with timestamp saved after episode 134: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-08-13.pth\n",
            "Episode 134/200, Total Reward: -2825.199999999971, Epsilon: 0.510849320360386\n",
            "Episode 135/200, Total Reward: -4161.399999999953, Epsilon: 0.5082950737585841\n",
            "Episode 136/200, Total Reward: -3942.0999999999513, Epsilon: 0.5057535983897912\n",
            "Episode 137/200, Total Reward: -4824.400000000001, Epsilon: 0.5032248303978422\n",
            "Episode 138/200, Total Reward: -4594.899999999984, Epsilon: 0.500708706245853\n",
            "Episode 139/200, Total Reward: -5467.000000000046, Epsilon: 0.4982051627146237\n",
            "Episode 140/200, Total Reward: -3528.9999999999586, Epsilon: 0.49571413690105054\n",
            "Episode 141/200, Total Reward: -3039.3999999999673, Epsilon: 0.4932355662165453\n",
            "Episode 142/200, Total Reward: -3008.799999999968, Epsilon: 0.4907693883854626\n",
            "Episode 143/200, Total Reward: -3365.7999999999615, Epsilon: 0.4883155414435353\n",
            "Episode 144/200, Total Reward: -3809.4999999999536, Epsilon: 0.4858739637363176\n",
            "Episode 145/200, Total Reward: -5278.300000000033, Epsilon: 0.483444593917636\n",
            "Episode 146/200, Total Reward: -3926.7999999999515, Epsilon: 0.4810273709480478\n",
            "Episode 147/200, Total Reward: -2350.8999999999796, Epsilon: 0.47862223409330756\n",
            "Episode 148/200, Total Reward: -4146.099999999952, Epsilon: 0.47622912292284103\n",
            "Episode 149/200, Total Reward: -4319.4999999999645, Epsilon: 0.4738479773082268\n",
            "Episode 150/200, Total Reward: -4722.399999999993, Epsilon: 0.47147873742168567\n",
            "Episode 151/200, Total Reward: -4140.999999999952, Epsilon: 0.46912134373457726\n",
            "Episode 152/200, Total Reward: -4242.999999999959, Epsilon: 0.46677573701590436\n",
            "Episode 153/200, Total Reward: -4304.199999999963, Epsilon: 0.46444185833082485\n",
            "Episode 154/200, Total Reward: -4538.79999999998, Epsilon: 0.46211964903917074\n",
            "Episode 155/200, Total Reward: -3304.5999999999626, Epsilon: 0.4598090507939749\n",
            "Episode 156/200, Total Reward: -3727.899999999955, Epsilon: 0.457510005540005\n",
            "Episode 157/200, Total Reward: -4732.599999999994, Epsilon: 0.45522245551230495\n",
            "Episode 158/200, Total Reward: -3110.799999999966, Epsilon: 0.4529463432347434\n",
            "Episode 159/200, Total Reward: -4069.599999999949, Epsilon: 0.4506816115185697\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-46-26.pth\n",
            "Model with timestamp saved after episode 160: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-46-26.pth\n",
            "Episode 160/200, Total Reward: -3120.999999999966, Epsilon: 0.4484282034609769\n",
            "Episode 161/200, Total Reward: -2371.2999999999793, Epsilon: 0.446186062443672\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-49-01.pth\n",
            "Model with timestamp saved after episode 162: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-49-01.pth\n",
            "Episode 162/200, Total Reward: -4278.699999999962, Epsilon: 0.4439551321314536\n",
            "Episode 163/200, Total Reward: -2029.5999999999851, Epsilon: 0.4417353564707963\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-50-49.pth\n",
            "Model with timestamp saved after episode 164: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-50-49.pth\n",
            "Episode 164/200, Total Reward: -2427.3999999999783, Epsilon: 0.43952667968844233\n",
            "Episode 165/200, Total Reward: -3666.699999999956, Epsilon: 0.43732904629000013\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-53-36.pth\n",
            "Model with timestamp saved after episode 166: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_22-53-36.pth\n",
            "Episode 166/200, Total Reward: -3666.699999999956, Epsilon: 0.4351424010585501\n",
            "Episode 167/200, Total Reward: -4151.1999999999525, Epsilon: 0.43296668905325736\n",
            "Episode 168/200, Total Reward: -3355.5999999999617, Epsilon: 0.43080185560799106\n",
            "Episode 169/200, Total Reward: -3977.7999999999506, Epsilon: 0.4286478463299511\n",
            "Episode 170/200, Total Reward: -5446.600000000045, Epsilon: 0.42650460709830135\n",
            "Episode 171/200, Total Reward: -3931.8999999999514, Epsilon: 0.42437208406280985\n",
            "Episode 172/200, Total Reward: -4309.299999999964, Epsilon: 0.4222502236424958\n",
            "Episode 173/200, Total Reward: -4130.799999999951, Epsilon: 0.42013897252428334\n",
            "Episode 174/200, Total Reward: -3972.6999999999507, Epsilon: 0.4180382776616619\n",
            "Episode 175/200, Total Reward: -4426.599999999972, Epsilon: 0.4159480862733536\n",
            "Episode 176/200, Total Reward: -2667.099999999974, Epsilon: 0.41386834584198684\n",
            "Episode 177/200, Total Reward: -3263.7999999999633, Epsilon: 0.4117990041127769\n",
            "Episode 178/200, Total Reward: -4610.199999999985, Epsilon: 0.40974000909221303\n",
            "Episode 179/200, Total Reward: -5120.200000000022, Epsilon: 0.40769130904675194\n",
            "Episode 180/200, Total Reward: -3340.299999999962, Epsilon: 0.40565285250151817\n",
            "Episode 181/200, Total Reward: -3131.1999999999657, Epsilon: 0.4036245882390106\n",
            "Episode 182/200, Total Reward: -4120.59999999995, Epsilon: 0.4016064652978155\n",
            "Episode 183/200, Total Reward: -3345.399999999962, Epsilon: 0.3995984329713264\n",
            "Episode 184/200, Total Reward: -4156.299999999953, Epsilon: 0.3976004408064698\n",
            "Episode 185/200, Total Reward: -2860.8999999999705, Epsilon: 0.39561243860243744\n",
            "Episode 186/200, Total Reward: -5038.600000000016, Epsilon: 0.3936343764094253\n",
            "Episode 187/200, Total Reward: -6288.100000000105, Epsilon: 0.39166620452737816\n",
            "Episode 188/200, Total Reward: -2039.799999999985, Epsilon: 0.3897078735047413\n",
            "Episode 189/200, Total Reward: -3860.4999999999527, Epsilon: 0.3877593341372176\n",
            "Episode 190/200, Total Reward: -3416.7999999999606, Epsilon: 0.3858205374665315\n",
            "Episode 191/200, Total Reward: -4406.199999999971, Epsilon: 0.38389143477919885\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-31-35.pth\n",
            "Model with timestamp saved after episode 192: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-31-35.pth\n",
            "Episode 192/200, Total Reward: -2911.8999999999696, Epsilon: 0.3819719776053028\n",
            "Episode 193/200, Total Reward: -3947.199999999951, Epsilon: 0.3800621177172763\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-34-48.pth\n",
            "Model with timestamp saved after episode 194: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-34-48.pth\n",
            "Episode 194/200, Total Reward: -4701.999999999992, Epsilon: 0.37816180712868996\n",
            "Episode 195/200, Total Reward: -3544.2999999999583, Epsilon: 0.37627099809304654\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-37-27.pth\n",
            "Model with timestamp saved after episode 196: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-37-27.pth\n",
            "Episode 196/200, Total Reward: -3294.399999999963, Epsilon: 0.3743896431025813\n",
            "Episode 197/200, Total Reward: -4452.099999999974, Epsilon: 0.37251769488706843\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-40-33.pth\n",
            "Model with timestamp saved after episode 198: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model_2024-12-10_19-20-01/PACMAN_dqn_model_2024-12-10_23-40-33.pth\n",
            "Episode 198/200, Total Reward: -3676.899999999956, Epsilon: 0.3706551064126331\n",
            "Episode 199/200, Total Reward: -3008.799999999968, Epsilon: 0.36880183088056995\n",
            "Episode 200/200, Total Reward: -4207.2999999999565, Epsilon: 0.3669578217261671\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Run Training\n",
        "train_pacman()\n",
        "#train_pacman_with_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN6fU6xeZ-kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725fa8bf-216b-4ba0-913d-84ea85b422fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/pacman_eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like to load a saved model? (yes/no): yes\n",
            "Enter the number of episodes to train the model: 10\n",
            "Enter the path to the saved model file: /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-6b5b1b8d1d5a>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)  # Load the checkpoint here\n",
            "<ipython-input-37-6b5b1b8d1d5a>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model with adjustments for 9 actions.\n",
            "Model loaded successfully.\n",
            "Loaded model from /content/drive/MyDrive/Colab Notebooks/Math5366 - Final Project/PACMAN_dqn_model.pth.\n",
            "Episode 1/10, Total Reward: 90.0\n",
            "Episode 2/10, Total Reward: 670.0\n",
            "Episode 3/10, Total Reward: 240.0\n",
            "Episode 4/10, Total Reward: 470.0\n",
            "Episode 5/10, Total Reward: 620.0\n",
            "Episode 6/10, Total Reward: 320.0\n",
            "Episode 7/10, Total Reward: 390.0\n",
            "Episode 8/10, Total Reward: 290.0\n",
            "Episode 9/10, Total Reward: 370.0\n",
            "Episode 10/10, Total Reward: 450.0\n",
            "Average reward over 10 episodes: 391.0\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Pacman environment with render_mode as \"rgb_array\"\n",
        "env = gym.make(\"MsPacman-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "input_shape = (4, 84, 84)  # Stack size × resized frame dimensions\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.00025\n",
        "epsilon = 0.1\n",
        "epsilon_min = 0\n",
        "epsilon_decay = 0.995\n",
        "batch_size = 32\n",
        "n_episodes = 10\n",
        "\n",
        "video_folder = '/content/pacman_eval_videos'  # Local folder to save videos\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "clear_folder(video_folder)\n",
        "\n",
        "# Track the highest reward and corresponding episode\n",
        "highest_reward = float('-inf')\n",
        "best_episode_video_folder = \"/content/best_pacman_eval_video\"\n",
        "os.makedirs(best_episode_video_folder, exist_ok=True)\n",
        "clear_folder(best_episode_video_folder)\n",
        "\n",
        "# Wrap the environment to record the video\n",
        "env = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
        "\n",
        "# Initialize list to track total rewards per episode\n",
        "total_test_rewards = []\n",
        "\n",
        "model = DQN(input_shape, n_actions)\n",
        "target_model = DQN(input_shape, n_actions)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "target_model.eval()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Load or create the model\n",
        "is_training = False\n",
        "model, target_model, n_eval_episodes, file_loc = initialize_model(input_shape, n_actions, optimizer, is_training)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "target_model.eval()\n",
        "\n",
        "# Check if GPU is available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# List of possible actions (adjust this if necessary)\n",
        "possible_actions = [0, 1, 2, 3, 4]  # For example\n",
        "steps = 0\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = preprocess_frame(state)  # Preprocess the initial state\n",
        "    stacked_state, frame_stack = stack_frames(None, state, stack_size=4)  # Stack the frames\n",
        "\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Model prediction\n",
        "        q_values = model(torch.FloatTensor(stacked_state).unsqueeze(0).to(device))\n",
        "        q_values = q_values.detach().cpu().numpy()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_values)\n",
        "\n",
        "        # Take action in the environment\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        done = done or truncated\n",
        "\n",
        "        # Preprocess and stack the next state\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        stacked_next_state, frame_stack = stack_frames(frame_stack, next_state, stack_size=4)\n",
        "\n",
        "        # Update the current state\n",
        "        stacked_state = stacked_next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode+1}/{n_eval_episodes}, Total Reward: {total_reward}\")\n",
        "    total_test_rewards.append(total_reward)\n",
        "env.close()\n",
        "\n",
        "# Compute and print evaluation metrics\n",
        "average_reward = np.mean(total_test_rewards)\n",
        "print(f\"Average reward over {n_eval_episodes} episodes: {average_reward}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}